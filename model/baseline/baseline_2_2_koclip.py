#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Baseline 2-2 KoClip Fine-tuning Model Evaluation Script

"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, Subset
import pandas as pd
import numpy as np
from PIL import Image
from tqdm import tqdm
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import accuracy_score, classification_report, mean_absolute_error, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
from transformers import AutoModel, AutoProcessor
import matplotlib.pyplot as plt
import seaborn as sns
import json
import warnings
warnings.filterwarnings('ignore')


class KoCLIPDataset(Dataset):
    """Custom dataset class for Korean CLIP"""
    def __init__(self, dataframe, image_folder, processor):
        self.data = dataframe
        self.image_folder = image_folder
        self.processor = processor

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        image_path = os.path.join(self.image_folder, row['Image Filename'])
        
        try:
            image = Image.open(image_path).convert("RGB")
        except Exception as e:
            print(f"Error loading image {image_path}: {e}")
            # 기본 이미지 생성 (검은색 이미지)
            image = Image.new('RGB', (224, 224), color='black')
            
        text = row["Hashtag"]
        label = int(row["Score"])

        inputs = self.processor(
            text=[text],
            images=image,
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=77
        )
        
        return {
            "pixel_values": inputs["pixel_values"].squeeze(0),
            "input_ids": inputs["input_ids"].squeeze(0),
            "attention_mask": inputs["attention_mask"].squeeze(0),
            "label": torch.tensor(label, dtype=torch.long)
        }


class FrozenKoCLIPWithClassifier(nn.Module):
    """Korean CLIP with frozen encoder and trainable classifier head"""
    def __init__(self, model_name="Bingsu/clip-vit-large-patch14-ko", num_classes=5):
        super(FrozenKoCLIPWithClassifier, self).__init__()
        self.clip_model = AutoModel.from_pretrained(model_name)
        
        # Freeze CLIP parameters
        for param in self.clip_model.parameters():
            param.requires_grad = False
            
        # Trainable classifier head
        self.classifier = nn.Sequential(
            nn.Linear(self.clip_model.config.projection_dim * 2, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )

    def forward(self, pixel_values, input_ids, attention_mask):
        with torch.no_grad():
            outputs = self.clip_model(
                pixel_values=pixel_values, 
                input_ids=input_ids, 
                attention_mask=attention_mask
            )
            image_features = outputs.image_embeds
            text_features = outputs.text_embeds
            
        combined = torch.cat((image_features, text_features), dim=1)
        return self.classifier(combined)


def setup_environment():
    """환경 설정"""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")
    
    if device == "cuda":
        print(f"GPU: {torch.cuda.get_device_name()}")
        print(f"CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    return device


def load_and_prepare_data(data_path='/Jupyter/Insta/data/combined_data.csv'):
    """데이터 로드 및 준비"""
    try:
        df = pd.read_csv(data_path)
        df['Score'] = df['Score'] - 1  # 0-4로 정규화
        print(f"Data loaded successfully. Shape: {df.shape}")
        print(f"Score distribution:\n{df['Score'].value_counts().sort_index()}")
        return df
    except Exception as e:
        print(f"Error loading data: {e}")
        raise


def train_model(train_dataset, val_dataset, device, config, output_dir, model_name):
    """단일 모델 학습"""
    print(f"\n[{model_name}] Training started")
    
    # 모델 초기화
    model = FrozenKoCLIPWithClassifier(num_classes=5).to(device)
    optimizer = optim.Adam(
        model.classifier.parameters(), 
        lr=config['learning_rate'], 
        weight_decay=config['weight_decay']
    )
    
    # 클래스 가중치 적용
    criterion = nn.CrossEntropyLoss(weight=config['class_weights'].to(device))
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)

    # 데이터 로더
    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'])

    # 학습 기록
    train_losses = []
    val_losses = []
    val_accuracies = []

    for epoch in range(config['epochs']):
        # 학습 단계
        model.train()
        train_loss = 0
        train_pbar = tqdm(train_loader, desc=f"{model_name}, Epoch {epoch+1}/{config['epochs']}")
        
        for batch in train_pbar:
            try:
                pixel_values = batch["pixel_values"].to(device)
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                labels = batch["label"].to(device)

                optimizer.zero_grad()
                outputs = model(pixel_values, input_ids, attention_mask)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})
                
            except Exception as e:
                print(f"Error in training batch: {e}")
                continue

        scheduler.step()

        # 검증 단계
        model.eval()
        val_loss = 0
        correct = 0
        total = 0
        all_labels = []
        all_predictions = []
        
        with torch.no_grad():
            for batch in val_loader:
                try:
                    pixel_values = batch["pixel_values"].to(device)
                    input_ids = batch["input_ids"].to(device)
                    attention_mask = batch["attention_mask"].to(device)
                    labels = batch["label"].to(device)

                    outputs = model(pixel_values, input_ids, attention_mask)
                    loss = criterion(outputs, labels)
                    val_loss += loss.item()

                    _, predicted = torch.max(outputs, 1)
                    correct += (predicted == labels).sum().item()
                    total += labels.size(0)
                    all_labels.extend(labels.cpu().numpy())
                    all_predictions.extend(predicted.cpu().numpy())
                    
                except Exception as e:
                    print(f"Error in validation batch: {e}")
                    continue

        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        val_accuracy = correct / total if total > 0 else 0

        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)
        val_accuracies.append(val_accuracy)

        print(f"{model_name}, Epoch {epoch + 1}/{config['epochs']}: "
              f"Train Loss: {avg_train_loss:.4f}, "
              f"Val Loss: {avg_val_loss:.4f}, "
              f"Val Accuracy: {val_accuracy:.4f}")
        
        # 매 에포크마다 분류 리포트 출력 (마지막 에포크에만)
        if epoch == config['epochs'] - 1:
            print("Final Classification Report:")
            print(classification_report(all_labels, all_predictions))

    # 모델 저장
    model_path = os.path.join(output_dir, f"{model_name.lower().replace(' ', '_')}_model.pth")
    torch.save(model.state_dict(), model_path)
    print(f"Full model saved: {model_path}")
    
    # 분류기만 저장
    classifier_path = os.path.join(output_dir, f"{model_name.lower().replace(' ', '_')}_classifier.pth")
    torch.save(model.classifier.state_dict(), classifier_path)
    print(f"Classifier saved: {classifier_path}")

    return model, all_labels, all_predictions, train_losses, val_losses, val_accuracies


def prepare_cross_validation(df, K=5):
    """교차 검증 준비"""
    skf = StratifiedKFold(n_splits=K, random_state=42, shuffle=True)
    labels = df['Score'].values
    fold_indices = list(skf.split(df, labels))
    
    print(f"Prepared {K}-fold cross validation")
    for i, (train_idx, val_idx) in enumerate(fold_indices):
        print(f"Fold {i+1}: Train={len(train_idx)}, Val={len(val_idx)}")
    
    return fold_indices, labels


def train_single_fold(fold, train_dataset, val_dataset, device, config, output_dir):
    """단일 폴드 학습"""
    model_name = f"Fold {fold + 1}/{config['K']}"
    model, fold_labels, fold_predictions, train_losses, val_losses, val_accuracies = train_model(
        train_dataset, val_dataset, device, config, output_dir, model_name
    )
    
    # 폴드별 모델 저장
    fold_model_path = os.path.join(output_dir, f"frozen_koclip_fold_{fold + 1}.pth")
    torch.save(model.state_dict(), fold_model_path)
    print(f"Fold {fold + 1} model saved: {fold_model_path}")
    
    return model, fold_labels, fold_predictions, train_losses, val_losses, val_accuracies


def evaluate_and_save_results(all_labels, all_predictions, method_name, output_dir):
    """결과 평가 및 저장"""
    # 메트릭 계산
    mae = mean_absolute_error(all_labels, all_predictions)
    acc = accuracy_score(all_labels, all_predictions)
    report = classification_report(all_labels, all_predictions)
    report_dict = classification_report(all_labels, all_predictions, output_dict=True)
    conf_matrix = confusion_matrix(all_labels, all_predictions)

    print(f"\n{method_name} Results:")
    print(f"MAE: {mae:.4f}")
    print(f"Accuracy: {acc:.4f}")
    print("\nClassification Report:")
    print(report)

    # 파일명 접두사
    prefix = method_name.lower().replace(' ', '_').replace('-', '_')

    # 혼동 행렬 시각화
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", 
                xticklabels=range(5), yticklabels=range(5))
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title(f"Confusion Matrix - {method_name}")
    plt.tight_layout()
    
    conf_path = os.path.join(output_dir, f"{prefix}_confusion_matrix.png")
    plt.savefig(conf_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Confusion matrix saved: {conf_path}")

    # CSV 파일 저장 (정량 지표)
    metrics_df = pd.DataFrame({
        'Method': [method_name],
        'MAE': [mae],
        'Accuracy': [acc],
        'Macro_F1': [report_dict['macro avg']['f1-score']],
        'Weighted_F1': [report_dict['weighted avg']['f1-score']]
    })
    csv_path = os.path.join(output_dir, f"{prefix}_evaluation_metrics.csv")
    metrics_df.to_csv(csv_path, index=False)
    print(f"Metrics saved: {csv_path}")

    # TXT 파일 저장 (전체 리포트)
    txt_path = os.path.join(output_dir, f"{prefix}_evaluation_metrics.txt")
    with open(txt_path, "w", encoding='utf-8') as f:
        f.write(f"{method_name} Results\n\n")
        f.write(f"MAE: {mae:.4f}\n")
        f.write(f"Accuracy: {acc:.4f}\n\n")
        f.write("Classification Report:\n")
        f.write(report)
    print(f"Results saved: {txt_path}")

    # JSON 파일 저장 (분류 리포트)
    json_path = os.path.join(output_dir, f"{prefix}_classification_report.json")
    with open(json_path, "w", encoding='utf-8') as f:
        json.dump(report_dict, f, indent=4, ensure_ascii=False)
    print(f"Classification report saved: {json_path}")

    return mae, acc, report_dict


def plot_training_curves(train_losses, val_losses, val_accuracies, method_name, output_dir):
    """학습 곡선 시각화"""
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    if isinstance(train_losses[0], list):  # Cross-validation case
        for fold in range(len(train_losses)):
            axes[0].plot(train_losses[fold], label=f'Fold {fold+1} Train', alpha=0.7)
            axes[1].plot(val_losses[fold], label=f'Fold {fold+1} Val', alpha=0.7)
            axes[2].plot(val_accuracies[fold], label=f'Fold {fold+1} Val Acc', alpha=0.7)
    else:  # Single split case
        axes[0].plot(train_losses, label='Train Loss', alpha=0.7)
        axes[1].plot(val_losses, label='Val Loss', alpha=0.7)
        axes[2].plot(val_accuracies, label='Val Accuracy', alpha=0.7)
    
    axes[0].set_title('Training Loss')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    axes[1].set_title('Validation Loss')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Loss')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    axes[2].set_title('Validation Accuracy')
    axes[2].set_xlabel('Epoch')
    axes[2].set_ylabel('Accuracy')
    axes[2].legend()
    axes[2].grid(True, alpha=0.3)
    
    plt.suptitle(f'{method_name} - Training Curves')
    plt.tight_layout()
    
    prefix = method_name.lower().replace(' ', '_').replace('-', '_')
    curves_path = os.path.join(output_dir, f"{prefix}_training_curves.png")
    plt.savefig(curves_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Training curves saved: {curves_path}")


def data_80_20_split(df, dataset, config, device, output_dir):
    """8:2 분할 학습 실행"""
    print("\n" + "="*60)
    print("8:2 SPLIT TRAINING")
    print("="*60)
    
    # 8:2 분할
    train_df, val_df = train_test_split(
        df, test_size=0.2, random_state=42, stratify=df['Score']
    )
    
    print(f"Train samples: {len(train_df)}, Validation samples: {len(val_df)}")
    
    # 데이터셋 생성
    train_indices = train_df.index.tolist()
    val_indices = val_df.index.tolist()
    
    train_dataset = Subset(dataset, train_indices)
    val_dataset = Subset(dataset, val_indices)
    
    # 모델 학습
    model, all_labels, all_predictions, train_losses, val_losses, val_accuracies = train_model(
        train_dataset, val_dataset, device, config, output_dir, "8:2 Split"
    )
    
    # 결과 평가 및 저장
    mae, acc, report_dict = evaluate_and_save_results(
        all_labels, all_predictions, "Baseline 2-2 8:2 Split", output_dir
    )
    
    # 학습 곡선 저장
    plot_training_curves(
        train_losses, val_losses, val_accuracies, "Baseline 2-2 8:2 Split", output_dir
    )
    
    return mae, acc, report_dict


def run_cross_validation(df, dataset, config, device, output_dir):
    """교차 검증 실행"""
    print("\n" + "="*60)
    print("K-FOLD CROSS-VALIDATION")
    print("="*60)
    
    # 교차 검증 준비
    fold_indices, labels = prepare_cross_validation(df, config['K'])
    
    dataset_splits = [
        (Subset(dataset, train_idx), Subset(dataset, val_idx)) 
        for train_idx, val_idx in fold_indices
    ]
    
    # 교차 검증 실행
    all_fold_labels = []
    all_fold_predictions = []
    all_train_losses = []
    all_val_losses = []
    all_val_accuracies = []
    
    for fold, (train_dataset, val_dataset) in enumerate(dataset_splits):
        model, fold_labels, fold_predictions, train_losses, val_losses, val_accuracies = train_single_fold(
            fold, train_dataset, val_dataset, device, config, output_dir
        )
        
        all_fold_labels.append(fold_labels)
        all_fold_predictions.append(fold_predictions)
        all_train_losses.append(train_losses)
        all_val_losses.append(val_losses)
        all_val_accuracies.append(val_accuracies)
        
        # GPU 메모리 정리
        if device == "cuda":
            torch.cuda.empty_cache()
    
    # 모든 폴드 결과 통합
    all_labels = []
    all_predictions = []
    
    for labels, predictions in zip(all_fold_labels, all_fold_predictions):
        all_labels.extend(labels)
        all_predictions.extend(predictions)
    
    # 최종 평가 및 결과 저장
    mae, acc, report_dict = evaluate_and_save_results(
        all_labels, all_predictions, "Baseline 2-2 Cross-Validation", output_dir
    )
    
    # 학습 곡선 저장
    plot_training_curves(
        all_train_losses, all_val_losses, all_val_accuracies, 
        "Baseline 2-2 Cross-Validation", output_dir
    )
    
    return mae, acc, report_dict


def main():
    """메인 실행 함수"""
    print("=" * 60)
    print("Baseline 2-2 Korean CLIP Fine-tuning Evaluation")
    print("Both 8:2 Split and K-Fold Cross-Validation")
    print("=" * 60)
    
    try:
        # 1. 환경 설정
        device = setup_environment()
        output_dir = "./results/"
        os.makedirs(output_dir, exist_ok=True)
        
        # 2. 설정
        config = {
            'K': 5,  # 폴드 수
            'learning_rate': 5e-5,
            'batch_size': 32,
            'epochs': 10,
            'weight_decay': 1e-4
        }
        
        # 3. 데이터 로드
        # TODO: 아래 경로를 본인의 데이터 경로로 수정하세요
        data_path = 'path/to/your/combined_data.csv'  # CSV 파일 경로
        image_folder = 'path/to/your/image/folder'    # 이미지 폴더 경로
        
        df = load_and_prepare_data(data_path)
        
        # 4. 클래스 가중치 계산
        labels = df['Score'].values
        class_weights = compute_class_weight(
            class_weight='balanced', 
            classes=np.unique(labels), 
            y=labels
        )
        config['class_weights'] = torch.tensor(class_weights, dtype=torch.float)
        print(f"Class weights: {class_weights}")
        
        # 5. 데이터셋 및 프로세서 준비
        print("Loading Korean CLIP processor...")
        processor = AutoProcessor.from_pretrained("Bingsu/clip-vit-large-patch14-ko")
        dataset = KoCLIPDataset(df, image_folder, processor)
        print("Dataset created successfully")
        
        # 6. 8:2 분할 실행
        mae_split, acc_split, report_split = data_80_20_split(
            df, dataset, config, device, output_dir
        )
        
        # 7. 교차 검증 실행
        mae_cv, acc_cv, report_cv = run_cross_validation(
            df, dataset, config, device, output_dir
        )
        
        # 8. 최종 비교 결과
        print("\n" + "="*60)
        print("FINAL COMPARISON")
        print("="*60)
        print(f"8:2 Split Results:")
        print(f"  MAE: {mae_split:.4f}")
        print(f"  Accuracy: {acc_split:.4f}")
        print(f"  Macro F1: {report_split['macro avg']['f1-score']:.4f}")
        print(f"  Weighted F1: {report_split['weighted avg']['f1-score']:.4f}")
        
        print(f"\nCross-Validation Results:")
        print(f"  MAE: {mae_cv:.4f}")
        print(f"  Accuracy: {acc_cv:.4f}")
        print(f"  Macro F1: {report_cv['macro avg']['f1-score']:.4f}")
        print(f"  Weighted F1: {report_cv['weighted avg']['f1-score']:.4f}")
        
        # 9. 종합 비교 결과 저장
        comparison_df = pd.DataFrame({
            'Method': ['8:2 Split', 'Cross-Validation'],
            'MAE': [mae_split, mae_cv],
            'Accuracy': [acc_split, acc_cv],
            'Macro_F1': [report_split['macro avg']['f1-score'], report_cv['macro avg']['f1-score']],
            'Weighted_F1': [report_split['weighted avg']['f1-score'], report_cv['weighted avg']['f1-score']]
        })
        
        comparison_path = os.path.join(output_dir, "baseline2_2_method_comparison.csv")
        comparison_df.to_csv(comparison_path, index=False)
        print(f"\nComparison results saved: {comparison_path}")
        
        print("="*60)
        print(f"All results saved in '{output_dir}' directory")
        
    except Exception as e:
        print(f"Error in main execution: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()